llm:
  provider: ollama         # Options: ollama, openai, groq, etc.
  model: llama3.1
  context_length: 60000    # Some models support higher limits

ollama:
  host: http://host.docker.internal:11434

# Notes:
# - To switch models or providers, edit the values above
# - You can add future providers (e.g., openai, anthropic) here under `llm`
# - Providers currently supported: groq, ollama