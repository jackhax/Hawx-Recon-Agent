llm:
  provider: groq         # Options: ollama, openai, groq, etc.
  model: meta-llama/llama-4-scout-17b-16e-instruct
  context_length: 60000    # Some models support higher limits

ollama:
  host: http://host.docker.internal:11434

# Notes:
# - To switch models or providers, edit the values above
# - You can add future providers (e.g., openai, anthropic) here under `llm`
# - Providers currently supported: groq, ollama